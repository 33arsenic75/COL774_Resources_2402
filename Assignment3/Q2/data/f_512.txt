Iteration 1, loss = 2.55294493
Iteration 2, loss = 1.90767154
Iteration 3, loss = 1.88605492
Iteration 4, loss = 1.87951202
Iteration 5, loss = 1.87534382
Iteration 6, loss = 1.87209626
Iteration 7, loss = 1.86937969
Iteration 8, loss = 1.86700515
Iteration 9, loss = 1.86485940
Iteration 10, loss = 1.86290951
Iteration 11, loss = 1.86110239
Iteration 12, loss = 1.85941184
Iteration 13, loss = 1.85782496
Iteration 14, loss = 1.85630742
Iteration 15, loss = 1.85487357
Iteration 16, loss = 1.85350524
Iteration 17, loss = 1.85218198
Iteration 18, loss = 1.85091249
Iteration 19, loss = 1.84969399
Iteration 20, loss = 1.84850635
Iteration 21, loss = 1.84736351
Iteration 22, loss = 1.84624952
Iteration 23, loss = 1.84516818
Iteration 24, loss = 1.84411046
Iteration 25, loss = 1.84308130
Iteration 26, loss = 1.84208577
Iteration 27, loss = 1.84109770
Iteration 28, loss = 1.84014411
Iteration 29, loss = 1.83920524
Iteration 30, loss = 1.83828361
Iteration 31, loss = 1.83738619
Iteration 32, loss = 1.83650065
Iteration 33, loss = 1.83562935
Iteration 34, loss = 1.83477763
Iteration 35, loss = 1.83393578
Iteration 36, loss = 1.83311112
Iteration 37, loss = 1.83229944
Iteration 38, loss = 1.83150199
Iteration 39, loss = 1.83071489
Iteration 40, loss = 1.82993980
Iteration 41, loss = 1.82917419
Iteration 42, loss = 1.82842398
Iteration 43, loss = 1.82767980
Iteration 44, loss = 1.82694696
Iteration 45, loss = 1.82622128
Iteration 46, loss = 1.82550737
Iteration 47, loss = 1.82480213
Iteration 48, loss = 1.82410346
Iteration 49, loss = 1.82341533
Iteration 50, loss = 1.82273343
Iteration 51, loss = 1.82206412
Iteration 52, loss = 1.82139743
Iteration 53, loss = 1.82074053
Iteration 54, loss = 1.82009127
Iteration 55, loss = 1.81944973
Iteration 56, loss = 1.81881008
Iteration 57, loss = 1.81817878
Iteration 58, loss = 1.81755565
Iteration 59, loss = 1.81693837
Iteration 60, loss = 1.81632765
Iteration 61, loss = 1.81572179
Iteration 62, loss = 1.81512010
Iteration 63, loss = 1.81452816
Iteration 64, loss = 1.81393728
Iteration 65, loss = 1.81335448
Iteration 66, loss = 1.81277557
Iteration 67, loss = 1.81219777
Iteration 68, loss = 1.81163114
Iteration 69, loss = 1.81106828
Iteration 70, loss = 1.81050829
Iteration 71, loss = 1.80995170
Iteration 72, loss = 1.80940119
Iteration 73, loss = 1.80885654
Iteration 74, loss = 1.80831684
Iteration 75, loss = 1.80777652
Iteration 76, loss = 1.80724376
Iteration 77, loss = 1.80671307
Iteration 78, loss = 1.80618773
Iteration 79, loss = 1.80566589
Iteration 80, loss = 1.80514795
Iteration 81, loss = 1.80463398
Iteration 82, loss = 1.80412368
Iteration 83, loss = 1.80361468
Iteration 84, loss = 1.80311086
Iteration 85, loss = 1.80261106
Iteration 86, loss = 1.80211567
Iteration 87, loss = 1.80162170
Iteration 88, loss = 1.80113151
Iteration 89, loss = 1.80064441
Iteration 90, loss = 1.80016012
Iteration 91, loss = 1.79967776
Iteration 92, loss = 1.79919938
Iteration 93, loss = 1.79872451
Iteration 94, loss = 1.79825255
Iteration 95, loss = 1.79778392
Iteration 96, loss = 1.79731587
Iteration 97, loss = 1.79685197
Iteration 98, loss = 1.79639107
Iteration 99, loss = 1.79593225
Iteration 100, loss = 1.79547725
Iteration 101, loss = 1.79502207
Iteration 102, loss = 1.79457228
Iteration 103, loss = 1.79412222
Iteration 104, loss = 1.79367778
Iteration 105, loss = 1.79323446
Iteration 106, loss = 1.79279331
Iteration 107, loss = 1.79235351
Iteration 108, loss = 1.79191778
Iteration 109, loss = 1.79148360
Iteration 110, loss = 1.79105103
Iteration 111, loss = 1.79062177
Iteration 112, loss = 1.79019472
Iteration 113, loss = 1.78977048
Iteration 114, loss = 1.78934632
Iteration 115, loss = 1.78892632
Iteration 116, loss = 1.78850556
Iteration 117, loss = 1.78809024
Iteration 118, loss = 1.78767428
Iteration 119, loss = 1.78726290
Iteration 120, loss = 1.78685288
Iteration 121, loss = 1.78644450
Iteration 122, loss = 1.78603676
Iteration 123, loss = 1.78563145
Iteration 124, loss = 1.78522916
Iteration 125, loss = 1.78482816
Iteration 126, loss = 1.78442696
Iteration 127, loss = 1.78403156
Iteration 128, loss = 1.78363239
Iteration 129, loss = 1.78324002
Iteration 130, loss = 1.78284683
Iteration 131, loss = 1.78245699
Iteration 132, loss = 1.78206738
Iteration 133, loss = 1.78168068
Iteration 134, loss = 1.78129348
Iteration 135, loss = 1.78091081
Iteration 136, loss = 1.78052671
Iteration 137, loss = 1.78014673
Iteration 138, loss = 1.77976698
Iteration 139, loss = 1.77939008
Iteration 140, loss = 1.77901374
Iteration 141, loss = 1.77863872
Iteration 142, loss = 1.77826549
Iteration 143, loss = 1.77789485
Iteration 144, loss = 1.77752408
Iteration 145, loss = 1.77715561
Iteration 146, loss = 1.77678845
Iteration 147, loss = 1.77642156
Iteration 148, loss = 1.77605838
Iteration 149, loss = 1.77569484
Iteration 150, loss = 1.77533395
Iteration 151, loss = 1.77497254
Iteration 152, loss = 1.77461388
Iteration 153, loss = 1.77425821
Iteration 154, loss = 1.77390041
Iteration 155, loss = 1.77354631
Iteration 156, loss = 1.77319182
Iteration 157, loss = 1.77284062
Iteration 158, loss = 1.77248801
Iteration 159, loss = 1.77213965
Iteration 160, loss = 1.77178944
Iteration 161, loss = 1.77144328
Iteration 162, loss = 1.77109868
Iteration 163, loss = 1.77075228
Iteration 164, loss = 1.77040734
Iteration 165, loss = 1.77006728
Iteration 166, loss = 1.76972545
Iteration 167, loss = 1.76938522
Iteration 168, loss = 1.76904681
Iteration 169, loss = 1.76870785
Iteration 170, loss = 1.76837147
Iteration 171, loss = 1.76803649
Iteration 172, loss = 1.76770310
Iteration 173, loss = 1.76736973
Iteration 174, loss = 1.76703781
Iteration 175, loss = 1.76670694
Iteration 176, loss = 1.76637577
Iteration 177, loss = 1.76604795
Iteration 178, loss = 1.76571902
Iteration 179, loss = 1.76539357
Iteration 180, loss = 1.76506657
Iteration 181, loss = 1.76474289
Iteration 182, loss = 1.76441869
Iteration 183, loss = 1.76409738
Iteration 184, loss = 1.76377417
Iteration 185, loss = 1.76345503
Iteration 186, loss = 1.76313476
Iteration 187, loss = 1.76281729
Iteration 188, loss = 1.76249818
Iteration 189, loss = 1.76218259
Iteration 190, loss = 1.76186690
Iteration 191, loss = 1.76155108
Iteration 192, loss = 1.76123794
Iteration 193, loss = 1.76092494
Iteration 194, loss = 1.76061294
Iteration 195, loss = 1.76030225
Iteration 196, loss = 1.75999186
Iteration 197, loss = 1.75968212
Iteration 198, loss = 1.75937486
Iteration 199, loss = 1.75906644
Iteration 200, loss = 1.75876094
hidden_layers: (512,)
Train Accuracy: 64.98%
Train Recall: 46.61%
Train Precision: 62.36%
Train F1: 47.61%
Test Accuracy: 60.26%
Test Recall: 41.09%
Test Precision: 53.66%
Test F1: 41.49%