Iteration 1, loss = 2.96578371
Iteration 2, loss = 2.10813617
Iteration 3, loss = 2.05566844
Iteration 4, loss = 2.04164522
Iteration 5, loss = 2.03278694
Iteration 6, loss = 2.02599945
Iteration 7, loss = 2.02041904
Iteration 8, loss = 2.01554014
Iteration 9, loss = 2.01109369
Iteration 10, loss = 2.00708723
Iteration 11, loss = 2.00333394
Iteration 12, loss = 1.99984346
Iteration 13, loss = 1.99650612
Iteration 14, loss = 1.99334043
Iteration 15, loss = 1.99037993
Iteration 16, loss = 1.98749261
Iteration 17, loss = 1.98473130
Iteration 18, loss = 1.98204303
Iteration 19, loss = 1.97949597
Iteration 20, loss = 1.97700155
Iteration 21, loss = 1.97458331
Iteration 22, loss = 1.97225541
Iteration 23, loss = 1.96993434
Iteration 24, loss = 1.96772540
Iteration 25, loss = 1.96554554
Iteration 26, loss = 1.96343152
Iteration 27, loss = 1.96134644
Iteration 28, loss = 1.95932703
Iteration 29, loss = 1.95734043
Iteration 30, loss = 1.95536879
Iteration 31, loss = 1.95345671
Iteration 32, loss = 1.95157457
Iteration 33, loss = 1.94972344
Iteration 34, loss = 1.94791382
Iteration 35, loss = 1.94612308
Iteration 36, loss = 1.94436969
Iteration 37, loss = 1.94263538
Iteration 38, loss = 1.94095500
Iteration 39, loss = 1.93925932
Iteration 40, loss = 1.93761808
Iteration 41, loss = 1.93597854
Iteration 42, loss = 1.93438844
Iteration 43, loss = 1.93280058
Iteration 44, loss = 1.93123124
Iteration 45, loss = 1.92968354
Iteration 46, loss = 1.92816086
Iteration 47, loss = 1.92665958
Iteration 48, loss = 1.92516889
Iteration 49, loss = 1.92368963
Iteration 50, loss = 1.92222815
Iteration 51, loss = 1.92078131
Iteration 52, loss = 1.91935528
Iteration 53, loss = 1.91793578
Iteration 54, loss = 1.91654199
Iteration 55, loss = 1.91516329
Iteration 56, loss = 1.91377670
Iteration 57, loss = 1.91243130
Iteration 58, loss = 1.91109987
Iteration 59, loss = 1.90975226
Iteration 60, loss = 1.90844103
Iteration 61, loss = 1.90713954
Iteration 62, loss = 1.90584765
Iteration 63, loss = 1.90455692
Iteration 64, loss = 1.90328631
Iteration 65, loss = 1.90203216
Iteration 66, loss = 1.90078544
Iteration 67, loss = 1.89955307
Iteration 68, loss = 1.89830739
Iteration 69, loss = 1.89709417
Iteration 70, loss = 1.89589689
Iteration 71, loss = 1.89468577
Iteration 72, loss = 1.89349544
Iteration 73, loss = 1.89232300
Iteration 74, loss = 1.89114412
Iteration 75, loss = 1.88998156
Iteration 76, loss = 1.88882463
Iteration 77, loss = 1.88767251
Iteration 78, loss = 1.88653810
Iteration 79, loss = 1.88540349
Iteration 80, loss = 1.88428317
Iteration 81, loss = 1.88316970
Iteration 82, loss = 1.88207400
Iteration 83, loss = 1.88096631
Iteration 84, loss = 1.87987001
Iteration 85, loss = 1.87878613
Iteration 86, loss = 1.87771182
Iteration 87, loss = 1.87663279
Iteration 88, loss = 1.87557932
Iteration 89, loss = 1.87451873
Iteration 90, loss = 1.87346407
Iteration 91, loss = 1.87242837
Iteration 92, loss = 1.87138737
Iteration 93, loss = 1.87035808
Iteration 94, loss = 1.86932341
Iteration 95, loss = 1.86829702
Iteration 96, loss = 1.86729627
Iteration 97, loss = 1.86627793
Iteration 98, loss = 1.86528320
Iteration 99, loss = 1.86429521
Iteration 100, loss = 1.86329989
Iteration 101, loss = 1.86231514
Iteration 102, loss = 1.86132957
Iteration 103, loss = 1.86035878
Iteration 104, loss = 1.85939036
Iteration 105, loss = 1.85842635
Iteration 106, loss = 1.85746500
Iteration 107, loss = 1.85651199
Iteration 108, loss = 1.85556142
Iteration 109, loss = 1.85461957
Iteration 110, loss = 1.85368727
Iteration 111, loss = 1.85274341
Iteration 112, loss = 1.85182204
Iteration 113, loss = 1.85088601
Iteration 114, loss = 1.84996859
Iteration 115, loss = 1.84905129
Iteration 116, loss = 1.84813270
Iteration 117, loss = 1.84723607
Iteration 118, loss = 1.84633674
Iteration 119, loss = 1.84542622
Iteration 120, loss = 1.84454327
Iteration 121, loss = 1.84364874
Iteration 122, loss = 1.84276397
Iteration 123, loss = 1.84189267
Iteration 124, loss = 1.84100912
Iteration 125, loss = 1.84014309
Iteration 126, loss = 1.83926728
Iteration 127, loss = 1.83839793
Iteration 128, loss = 1.83754040
Iteration 129, loss = 1.83667976
Iteration 130, loss = 1.83582694
Iteration 131, loss = 1.83497529
Iteration 132, loss = 1.83412381
Iteration 133, loss = 1.83329263
Iteration 134, loss = 1.83245231
Iteration 135, loss = 1.83161329
Iteration 136, loss = 1.83078136
Iteration 137, loss = 1.82995268
Iteration 138, loss = 1.82912048
Iteration 139, loss = 1.82830542
Iteration 140, loss = 1.82749257
Iteration 141, loss = 1.82667763
Iteration 142, loss = 1.82585173
Iteration 143, loss = 1.82505481
Iteration 144, loss = 1.82425116
Iteration 145, loss = 1.82344484
Iteration 146, loss = 1.82264763
Iteration 147, loss = 1.82185397
Iteration 148, loss = 1.82106223
Iteration 149, loss = 1.82027585
Iteration 150, loss = 1.81948497
Iteration 151, loss = 1.81869483
Iteration 152, loss = 1.81791937
Iteration 153, loss = 1.81714882
Iteration 154, loss = 1.81637186
Iteration 155, loss = 1.81559192
Iteration 156, loss = 1.81483319
Iteration 157, loss = 1.81406241
Iteration 158, loss = 1.81330142
Iteration 159, loss = 1.81254293
Iteration 160, loss = 1.81177928
Iteration 161, loss = 1.81103206
Iteration 162, loss = 1.81027563
Iteration 163, loss = 1.80952575
Iteration 164, loss = 1.80877832
Iteration 165, loss = 1.80803271
Iteration 166, loss = 1.80729909
Iteration 167, loss = 1.80655405
Iteration 168, loss = 1.80582237
Iteration 169, loss = 1.80508844
Iteration 170, loss = 1.80435392
Iteration 171, loss = 1.80362293
Iteration 172, loss = 1.80289800
Iteration 173, loss = 1.80217484
Iteration 174, loss = 1.80145006
Iteration 175, loss = 1.80072722
Iteration 176, loss = 1.80001419
Iteration 177, loss = 1.79930189
Iteration 178, loss = 1.79858770
Iteration 179, loss = 1.79786257
Iteration 180, loss = 1.79717209
Iteration 181, loss = 1.79645953
Iteration 182, loss = 1.79575589
Iteration 183, loss = 1.79505746
Iteration 184, loss = 1.79435746
Iteration 185, loss = 1.79366306
Iteration 186, loss = 1.79296963
Iteration 187, loss = 1.79227623
Iteration 188, loss = 1.79157834
Iteration 189, loss = 1.79089134
Iteration 190, loss = 1.79020947
Iteration 191, loss = 1.78952576
Iteration 192, loss = 1.78883821
Iteration 193, loss = 1.78816361
Iteration 194, loss = 1.78748280
Iteration 195, loss = 1.78680592
Iteration 196, loss = 1.78613359
Iteration 197, loss = 1.78546367
Iteration 198, loss = 1.78479341
Iteration 199, loss = 1.78412657
Iteration 200, loss = 1.78345875
hidden_layers: (512, 256, 128, 64)
Train Accuracy: 53.93%
Train Recall: 33.17%
Train Precision: 42.95%
Train F1: 30.79%
Test Accuracy: 50.18%
Test Recall: 30.14%
Test Precision: 31.32%
Test F1: 27.90%