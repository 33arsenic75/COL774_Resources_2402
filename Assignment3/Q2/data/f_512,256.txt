Iteration 1, loss = 2.55237770
Iteration 2, loss = 1.80544738
Iteration 3, loss = 1.75300980
Iteration 4, loss = 1.74351608
Iteration 5, loss = 1.73788081
Iteration 6, loss = 1.73368837
Iteration 7, loss = 1.73022491
Iteration 8, loss = 1.72723513
Iteration 9, loss = 1.72452975
Iteration 10, loss = 1.72205048
Iteration 11, loss = 1.71975337
Iteration 12, loss = 1.71760875
Iteration 13, loss = 1.71556729
Iteration 14, loss = 1.71364704
Iteration 15, loss = 1.71179810
Iteration 16, loss = 1.71003382
Iteration 17, loss = 1.70833390
Iteration 18, loss = 1.70670031
Iteration 19, loss = 1.70511477
Iteration 20, loss = 1.70357756
Iteration 21, loss = 1.70209645
Iteration 22, loss = 1.70065227
Iteration 23, loss = 1.69925314
Iteration 24, loss = 1.69787906
Iteration 25, loss = 1.69654199
Iteration 26, loss = 1.69524793
Iteration 27, loss = 1.69397363
Iteration 28, loss = 1.69272628
Iteration 29, loss = 1.69150540
Iteration 30, loss = 1.69030583
Iteration 31, loss = 1.68913636
Iteration 32, loss = 1.68798715
Iteration 33, loss = 1.68685793
Iteration 34, loss = 1.68574822
Iteration 35, loss = 1.68465611
Iteration 36, loss = 1.68358185
Iteration 37, loss = 1.68252488
Iteration 38, loss = 1.68148428
Iteration 39, loss = 1.68045828
Iteration 40, loss = 1.67944686
Iteration 41, loss = 1.67845173
Iteration 42, loss = 1.67746916
Iteration 43, loss = 1.67650515
Iteration 44, loss = 1.67555026
Iteration 45, loss = 1.67460725
Iteration 46, loss = 1.67368021
Iteration 47, loss = 1.67275614
Iteration 48, loss = 1.67184866
Iteration 49, loss = 1.67095272
Iteration 50, loss = 1.67006736
Iteration 51, loss = 1.66918948
Iteration 52, loss = 1.66831948
Iteration 53, loss = 1.66746071
Iteration 54, loss = 1.66661301
Iteration 55, loss = 1.66577824
Iteration 56, loss = 1.66494774
Iteration 57, loss = 1.66412471
Iteration 58, loss = 1.66331055
Iteration 59, loss = 1.66250329
Iteration 60, loss = 1.66170460
Iteration 61, loss = 1.66090702
Iteration 62, loss = 1.66012970
Iteration 63, loss = 1.65935310
Iteration 64, loss = 1.65858120
Iteration 65, loss = 1.65782214
Iteration 66, loss = 1.65706334
Iteration 67, loss = 1.65631513
Iteration 68, loss = 1.65557252
Iteration 69, loss = 1.65483323
Iteration 70, loss = 1.65410266
Iteration 71, loss = 1.65337894
Iteration 72, loss = 1.65265755
Iteration 73, loss = 1.65194737
Iteration 74, loss = 1.65123568
Iteration 75, loss = 1.65053773
Iteration 76, loss = 1.64983986
Iteration 77, loss = 1.64914962
Iteration 78, loss = 1.64846097
Iteration 79, loss = 1.64777691
Iteration 80, loss = 1.64710268
Iteration 81, loss = 1.64643302
Iteration 82, loss = 1.64576769
Iteration 83, loss = 1.64510332
Iteration 84, loss = 1.64444732
Iteration 85, loss = 1.64379337
Iteration 86, loss = 1.64314368
Iteration 87, loss = 1.64250088
Iteration 88, loss = 1.64185857
Iteration 89, loss = 1.64122169
Iteration 90, loss = 1.64058627
Iteration 91, loss = 1.63996316
Iteration 92, loss = 1.63933568
Iteration 93, loss = 1.63871687
Iteration 94, loss = 1.63809745
Iteration 95, loss = 1.63748766
Iteration 96, loss = 1.63687868
Iteration 97, loss = 1.63627237
Iteration 98, loss = 1.63567250
Iteration 99, loss = 1.63507206
Iteration 100, loss = 1.63447541
Iteration 101, loss = 1.63388566
Iteration 102, loss = 1.63329644
Iteration 103, loss = 1.63271309
Iteration 104, loss = 1.63213290
Iteration 105, loss = 1.63155313
Iteration 106, loss = 1.63097845
Iteration 107, loss = 1.63040602
Iteration 108, loss = 1.62983581
Iteration 109, loss = 1.62927044
Iteration 110, loss = 1.62870601
Iteration 111, loss = 1.62815019
Iteration 112, loss = 1.62758938
Iteration 113, loss = 1.62703604
Iteration 114, loss = 1.62648535
Iteration 115, loss = 1.62593621
Iteration 116, loss = 1.62539064
Iteration 117, loss = 1.62484912
Iteration 118, loss = 1.62430749
Iteration 119, loss = 1.62377115
Iteration 120, loss = 1.62323538
Iteration 121, loss = 1.62270612
Iteration 122, loss = 1.62217286
Iteration 123, loss = 1.62164459
Iteration 124, loss = 1.62111986
Iteration 125, loss = 1.62059565
Iteration 126, loss = 1.62007904
Iteration 127, loss = 1.61955813
Iteration 128, loss = 1.61904186
Iteration 129, loss = 1.61852974
Iteration 130, loss = 1.61801948
Iteration 131, loss = 1.61750973
Iteration 132, loss = 1.61700521
Iteration 133, loss = 1.61649875
Iteration 134, loss = 1.61599938
Iteration 135, loss = 1.61549861
Iteration 136, loss = 1.61500144
Iteration 137, loss = 1.61450499
Iteration 138, loss = 1.61401353
Iteration 139, loss = 1.61351914
Iteration 140, loss = 1.61303307
Iteration 141, loss = 1.61254490
Iteration 142, loss = 1.61205849
Iteration 143, loss = 1.61157779
Iteration 144, loss = 1.61109667
Iteration 145, loss = 1.61061430
Iteration 146, loss = 1.61013803
Iteration 147, loss = 1.60966285
Iteration 148, loss = 1.60919015
Iteration 149, loss = 1.60871641
Iteration 150, loss = 1.60824711
Iteration 151, loss = 1.60778057
Iteration 152, loss = 1.60731327
Iteration 153, loss = 1.60684842
Iteration 154, loss = 1.60638512
Iteration 155, loss = 1.60592494
Iteration 156, loss = 1.60546702
Iteration 157, loss = 1.60501034
Iteration 158, loss = 1.60455458
Iteration 159, loss = 1.60409810
Iteration 160, loss = 1.60364504
Iteration 161, loss = 1.60319672
Iteration 162, loss = 1.60274864
Iteration 163, loss = 1.60229921
Iteration 164, loss = 1.60185272
Iteration 165, loss = 1.60140869
Iteration 166, loss = 1.60096633
Iteration 167, loss = 1.60052462
Iteration 168, loss = 1.60008492
Iteration 169, loss = 1.59964710
Iteration 170, loss = 1.59920779
Iteration 171, loss = 1.59877455
Iteration 172, loss = 1.59833990
Iteration 173, loss = 1.59790900
Iteration 174, loss = 1.59747765
Iteration 175, loss = 1.59704916
Iteration 176, loss = 1.59662250
Iteration 177, loss = 1.59619487
Iteration 178, loss = 1.59577035
Iteration 179, loss = 1.59534630
Iteration 180, loss = 1.59492446
Iteration 181, loss = 1.59450389
Iteration 182, loss = 1.59408473
Iteration 183, loss = 1.59366478
Iteration 184, loss = 1.59324870
Iteration 185, loss = 1.59283254
Iteration 186, loss = 1.59242095
Iteration 187, loss = 1.59200482
Iteration 188, loss = 1.59159603
Iteration 189, loss = 1.59118339
Iteration 190, loss = 1.59077430
Iteration 191, loss = 1.59036776
Iteration 192, loss = 1.58996154
Iteration 193, loss = 1.58955744
Iteration 194, loss = 1.58915341
Iteration 195, loss = 1.58875072
Iteration 196, loss = 1.58834744
Iteration 197, loss = 1.58794966
Iteration 198, loss = 1.58754872
Iteration 199, loss = 1.58715280
Iteration 200, loss = 1.58675379
hidden_layers: (512, 256)
Train Accuracy: 65.90%
Train Recall: 48.71%
Train Precision: 63.72%
Train F1: 50.53%
Test Accuracy: 60.22%
Test Recall: 42.03%
Test Precision: 53.81%
Test F1: 42.88%