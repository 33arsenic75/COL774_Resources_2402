Iteration 1, loss = 2.70906221
Iteration 2, loss = 1.81437356
Iteration 3, loss = 1.74828446
Iteration 4, loss = 1.73679983
Iteration 5, loss = 1.72999255
Iteration 6, loss = 1.72480284
Iteration 7, loss = 1.72045958
Iteration 8, loss = 1.71656585
Iteration 9, loss = 1.71309391
Iteration 10, loss = 1.70991331
Iteration 11, loss = 1.70692306
Iteration 12, loss = 1.70411749
Iteration 13, loss = 1.70145281
Iteration 14, loss = 1.69897405
Iteration 15, loss = 1.69656813
Iteration 16, loss = 1.69427072
Iteration 17, loss = 1.69206439
Iteration 18, loss = 1.68993275
Iteration 19, loss = 1.68787170
Iteration 20, loss = 1.68587951
Iteration 21, loss = 1.68393275
Iteration 22, loss = 1.68206085
Iteration 23, loss = 1.68024232
Iteration 24, loss = 1.67844797
Iteration 25, loss = 1.67673170
Iteration 26, loss = 1.67503019
Iteration 27, loss = 1.67336974
Iteration 28, loss = 1.67174469
Iteration 29, loss = 1.67016981
Iteration 30, loss = 1.66861543
Iteration 31, loss = 1.66708654
Iteration 32, loss = 1.66558610
Iteration 33, loss = 1.66412364
Iteration 34, loss = 1.66268987
Iteration 35, loss = 1.66126424
Iteration 36, loss = 1.65986744
Iteration 37, loss = 1.65849263
Iteration 38, loss = 1.65714745
Iteration 39, loss = 1.65580808
Iteration 40, loss = 1.65450471
Iteration 41, loss = 1.65319653
Iteration 42, loss = 1.65193226
Iteration 43, loss = 1.65066891
Iteration 44, loss = 1.64942118
Iteration 45, loss = 1.64820906
Iteration 46, loss = 1.64699810
Iteration 47, loss = 1.64580751
Iteration 48, loss = 1.64462064
Iteration 49, loss = 1.64346472
Iteration 50, loss = 1.64231161
Iteration 51, loss = 1.64117349
Iteration 52, loss = 1.64005289
Iteration 53, loss = 1.63893404
Iteration 54, loss = 1.63783291
Iteration 55, loss = 1.63674549
Iteration 56, loss = 1.63566369
Iteration 57, loss = 1.63459198
Iteration 58, loss = 1.63353877
Iteration 59, loss = 1.63249582
Iteration 60, loss = 1.63145880
Iteration 61, loss = 1.63043357
Iteration 62, loss = 1.62941485
Iteration 63, loss = 1.62841151
Iteration 64, loss = 1.62741640
Iteration 65, loss = 1.62642443
Iteration 66, loss = 1.62545120
Iteration 67, loss = 1.62447977
Iteration 68, loss = 1.62351605
Iteration 69, loss = 1.62255913
Iteration 70, loss = 1.62161273
Iteration 71, loss = 1.62067568
Iteration 72, loss = 1.61975354
Iteration 73, loss = 1.61882582
Iteration 74, loss = 1.61791343
Iteration 75, loss = 1.61699700
Iteration 76, loss = 1.61610343
Iteration 77, loss = 1.61519924
Iteration 78, loss = 1.61431496
Iteration 79, loss = 1.61343259
Iteration 80, loss = 1.61256033
Iteration 81, loss = 1.61169719
Iteration 82, loss = 1.61082443
Iteration 83, loss = 1.60997575
Iteration 84, loss = 1.60912363
Iteration 85, loss = 1.60827781
Iteration 86, loss = 1.60744096
Iteration 87, loss = 1.60660546
Iteration 88, loss = 1.60577507
Iteration 89, loss = 1.60495292
Iteration 90, loss = 1.60413716
Iteration 91, loss = 1.60332268
Iteration 92, loss = 1.60251658
Iteration 93, loss = 1.60171541
Iteration 94, loss = 1.60092074
Iteration 95, loss = 1.60012874
Iteration 96, loss = 1.59933573
Iteration 97, loss = 1.59855610
Iteration 98, loss = 1.59777830
Iteration 99, loss = 1.59700384
Iteration 100, loss = 1.59624269
Iteration 101, loss = 1.59548255
Iteration 102, loss = 1.59471827
Iteration 103, loss = 1.59396181
Iteration 104, loss = 1.59321214
Iteration 105, loss = 1.59246429
Iteration 106, loss = 1.59172649
Iteration 107, loss = 1.59098997
Iteration 108, loss = 1.59025717
Iteration 109, loss = 1.58952278
Iteration 110, loss = 1.58879620
Iteration 111, loss = 1.58807821
Iteration 112, loss = 1.58735595
Iteration 113, loss = 1.58664344
Iteration 114, loss = 1.58593872
Iteration 115, loss = 1.58522651
Iteration 116, loss = 1.58451854
Iteration 117, loss = 1.58382110
Iteration 118, loss = 1.58312536
Iteration 119, loss = 1.58243946
Iteration 120, loss = 1.58174448
Iteration 121, loss = 1.58106234
Iteration 122, loss = 1.58037682
Iteration 123, loss = 1.57969854
Iteration 124, loss = 1.57902268
Iteration 125, loss = 1.57835171
Iteration 126, loss = 1.57768142
Iteration 127, loss = 1.57701479
Iteration 128, loss = 1.57634857
Iteration 129, loss = 1.57569173
Iteration 130, loss = 1.57503198
Iteration 131, loss = 1.57438122
Iteration 132, loss = 1.57372984
Iteration 133, loss = 1.57307769
Iteration 134, loss = 1.57243163
Iteration 135, loss = 1.57179139
Iteration 136, loss = 1.57114713
Iteration 137, loss = 1.57051258
Iteration 138, loss = 1.56987855
Iteration 139, loss = 1.56924972
Iteration 140, loss = 1.56861732
Iteration 141, loss = 1.56798768
Iteration 142, loss = 1.56736621
Iteration 143, loss = 1.56674719
Iteration 144, loss = 1.56612310
Iteration 145, loss = 1.56551025
Iteration 146, loss = 1.56489787
Iteration 147, loss = 1.56428561
Iteration 148, loss = 1.56367921
Iteration 149, loss = 1.56306657
Iteration 150, loss = 1.56246922
Iteration 151, loss = 1.56186593
Iteration 152, loss = 1.56126508
Iteration 153, loss = 1.56066980
Iteration 154, loss = 1.56007446
Iteration 155, loss = 1.55948041
Iteration 156, loss = 1.55889215
Iteration 157, loss = 1.55830664
Iteration 158, loss = 1.55772394
Iteration 159, loss = 1.55714092
Iteration 160, loss = 1.55655914
Iteration 161, loss = 1.55598283
Iteration 162, loss = 1.55540554
Iteration 163, loss = 1.55483271
Iteration 164, loss = 1.55425836
Iteration 165, loss = 1.55368827
Iteration 166, loss = 1.55312319
Iteration 167, loss = 1.55255764
Iteration 168, loss = 1.55199213
Iteration 169, loss = 1.55143273
Iteration 170, loss = 1.55086954
Iteration 171, loss = 1.55031160
Iteration 172, loss = 1.54975400
Iteration 173, loss = 1.54920391
Iteration 174, loss = 1.54865108
Iteration 175, loss = 1.54810236
Iteration 176, loss = 1.54755273
Iteration 177, loss = 1.54701024
Iteration 178, loss = 1.54646513
Iteration 179, loss = 1.54592302
Iteration 180, loss = 1.54538612
Iteration 181, loss = 1.54484843
Iteration 182, loss = 1.54431129
Iteration 183, loss = 1.54377302
Iteration 184, loss = 1.54324414
Iteration 185, loss = 1.54271206
Iteration 186, loss = 1.54217974
Iteration 187, loss = 1.54165416
Iteration 188, loss = 1.54112730
Iteration 189, loss = 1.54060498
Iteration 190, loss = 1.54008289
Iteration 191, loss = 1.53956062
Iteration 192, loss = 1.53904414
Iteration 193, loss = 1.53852419
Iteration 194, loss = 1.53801033
Iteration 195, loss = 1.53749647
Iteration 196, loss = 1.53698364
Iteration 197, loss = 1.53647018
Iteration 198, loss = 1.53596354
Iteration 199, loss = 1.53545555
Iteration 200, loss = 1.53494659
hidden_layers: (512, 256, 128)
Train Accuracy: 63.10%
Train Recall: 47.23%
Train Precision: 60.18%
Train F1: 48.55%
Test Accuracy: 58.50%
Test Recall: 40.62%
Test Precision: 50.94%
Test F1: 40.90%